{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e90ee602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consent-button clicked.\n",
      "2nd consent-button clicked.\n",
      "Login successful\n",
      "Page: 240 of 252\n",
      "Page scrape time: 32 minutes\n",
      "Page: 241 of 252\n",
      "Page scrape time: 25 minutes\n",
      "Page: 242 of 252\n",
      "Page scrape time: 25 minutes\n",
      "Page: 243 of 252\n",
      "Page scrape time: 26 minutes\n",
      "Page: 244 of 252\n",
      "Page scrape time: 25 minutes\n",
      "Page: 245 of 252\n",
      "Page scrape time: 25 minutes\n",
      "Page: 246 of 252\n",
      "Page scrape time: 25 minutes\n",
      "Page: 247 of 252\n",
      "Page scrape time: 26 minutes\n",
      "Page: 248 of 252\n",
      "Page scrape time: 26 minutes\n",
      "Page: 249 of 252\n",
      "Page scrape time: 26 minutes\n",
      "Page: 250 of 252\n",
      "Page scrape time: 25 minutes\n",
      "Page: 251 of 252\n",
      "Page scrape time: 26 minutes\n",
      "Page: 252 of 252\n",
      "Page scrape time: 25 minutes\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# login page\n",
    "login_url = 'https://boardgamegeek.com/login?redirect_server=1'  \n",
    "\n",
    "# create an instance of the Chrome WebDriver with Selenium\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the login page\n",
    "driver.get(login_url)\n",
    "\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "try:\n",
    "    # Wait for the consent button to become clickable\n",
    "    consent_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((\n",
    "            By.XPATH,\n",
    "            \"//button[@aria-label=\\\"I'm OK with that\\\"]\"\n",
    "        ))\n",
    "    )\n",
    "    consent_button.click()\n",
    "    print(\"Consent-button clicked.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to click consent-button:\", e)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "\n",
    "# finding the next cookie pop-up question and clicking \"OK\"\n",
    "try:\n",
    "    cookie_button = driver.find_element(By.XPATH, '//button[contains(text(), \"I\\'m OK with that\")]')\n",
    "    cookie_button.click()\n",
    "    print(\"2nd consent-button clicked.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to click 2nd consent-button:\", e)\n",
    "\n",
    "WebDriverWait(driver, 20).until(EC.staleness_of(driver.find_element(By.TAG_NAME, 'html')))\n",
    "\n",
    "\n",
    "# Locate the username and password input fields and the sign-in button using their HTML attributes\n",
    "username_input = driver.find_element(By.ID, 'inputUsername')  \n",
    "password_input = driver.find_element(By.ID, 'inputPassword')  \n",
    "signin_button = driver.find_element(By.XPATH, '//button[contains(text(), \"Sign In\")]') \n",
    "\n",
    "# Input my username and password\n",
    "username_input.send_keys('fotbill13')  \n",
    "password_input.send_keys('panatha1') \n",
    "driver.implicitly_wait(5)\n",
    "# Submit the login form by clicking the sign-in button\n",
    "signin_button.click()\n",
    "\n",
    "# Wait until the username/profile element is visible\n",
    "WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href*=\"/user/\"]'))\n",
    ")\n",
    "print(\"Login successful\")\n",
    "\n",
    "# Fetch content from a page after login\n",
    "driver.get('https://boardgamegeek.com/browse/boardgame/page/')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========== LOOPING THE PAGES ==========\n",
    "# Starting the for-loop for a desired range of pages of 100 board games each\n",
    "r = range(240, 253)\n",
    "for i in r:\n",
    "    print('Page:', i, 'of', r[-1])\n",
    "    page_start_time = time.time()\n",
    "    resx = driver.get('https://boardgamegeek.com/browse/boardgame/page/'+str(i))\n",
    "    time.sleep(5)  # inserting a 5 second pause due to the web pages crawl policy\n",
    "    soupx = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    #  Get the titles of the board games\n",
    "    titles = soupx.select('.primary')\n",
    "    titles_list = [titles[k].get_text() for k in range(len(titles))]\n",
    "    \n",
    "    # Get the geek rating, average rating and number of voters\n",
    "    ratingsx = soupx.select('.collection_bggrating')\n",
    "    td_elementsx = soupx.find_all('td', class_='collection_bggrating', align='center')\n",
    "    td_listx = []\n",
    "    for td in td_elementsx:\n",
    "        number = td.get_text().strip() \n",
    "        td_listx.append(number)\n",
    "    ratings_listx = []\n",
    "    for td_list_i in td_listx:\n",
    "        if td_list_i == 'N/A':\n",
    "            ratings_listx.append(td_list_i)\n",
    "        else:\n",
    "            ratings_listx.append(float(td_list_i))\n",
    "    sublistsx = []\n",
    "    for j in range(0, len(ratings_listx), 3):\n",
    "        sublistx = ratings_listx[j:j + 3]\n",
    "        sublistsx.append(sublistx)\n",
    "    geek_rating = [inner_list[0] for inner_list in sublistsx]\n",
    "    avg_rating = [inner_list[1] for inner_list in sublistsx]\n",
    "    voters = [inner_list[2] if inner_list[2]=='N/A' else int(inner_list[2]) for inner_list in sublistsx]\n",
    "\n",
    "\n",
    "\n",
    "    # ====== PRICE ====== \n",
    "    # Get all td blocks that might contain shop prices\n",
    "    tds = soupx.find_all('td', class_='collection_shop')\n",
    "\n",
    "    price_list = []\n",
    "    for td in tds:\n",
    "        amazon_tag = td.find('a', class_='ulprice', href=True)\n",
    "        if amazon_tag and 'amazon.com' in amazon_tag['href']:\n",
    "            price_span = amazon_tag.find('span', class_='positive')\n",
    "            if price_span:\n",
    "                price_text = price_span.text.strip().replace('$', '').replace(',', '')\n",
    "                try:\n",
    "                    price = float(price_text)\n",
    "                    price_list.append(price)\n",
    "                except ValueError:\n",
    "                    price_list.append('N/A')\n",
    "            else:\n",
    "                price_list.append('N/A')\n",
    "        else:\n",
    "            price_list.append('N/A')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- Individual board game pages ------------\n",
    "    # Get the links of each Board Game in the current page       \n",
    "    links = soupx.find_all('a', class_='primary')\n",
    "    link_urls = []\n",
    "    for link in links:\n",
    "        link_href = link.get('href')\n",
    "        link_urls.append(link_href)\n",
    "\n",
    "    # Initialize the lists for the 100 board games of the current page\n",
    "    year_list = []\n",
    "    weight_list = []\n",
    "    min_players_list = []\n",
    "    max_players_list = []\n",
    "    min_time_list = []\n",
    "    max_time_list = []\n",
    "    age_list  = []\n",
    "    type1_list = []\n",
    "    type2_list = []\n",
    "    designer_rows = []\n",
    "    artist_rows = []\n",
    "    publisher_rows = []\n",
    "\n",
    "\n",
    "    # Looping through each of the 100 board game links of each page\n",
    "    for link in link_urls:\n",
    "        driver.get(f'https://boardgamegeek.com{link}')\n",
    "        time.sleep(10)     \n",
    "        soup2 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "        # ====== YEAR ====== \n",
    "        year = soup2.find('span', class_ = 'game-year ng-binding ng-scope')\n",
    "        if year:\n",
    "            year = year.get_text(strip=True).strip('()')\n",
    "            year_list.append(int(year))\n",
    "        else:\n",
    "            year_list.append('N/A')\n",
    "        \n",
    "\n",
    "        # ====== COMPLEXITY (WEIGHT) ====== \n",
    "        weight = 'N/A'\n",
    "        # Select all span elements that include ng-binding and any gameplay-weight class\n",
    "        weight_spans = soup2.select('span.ng-binding[class*=\"gameplay-weight-\"]')\n",
    "        if weight_spans:\n",
    "            try:\n",
    "                weight = float(weight_spans[0].text.strip())\n",
    "            except ValueError:\n",
    "                pass\n",
    "        weight_list.append(weight)\n",
    "\n",
    "\n",
    "        # Find all <p> tags with the class (because there are many: Players, Age, Playtime etc.)\n",
    "        p_tags = soup2.find_all('p', class_='gameplay-item-primary mb-0')\n",
    "\n",
    "\n",
    "        # ====== AGE ====== \n",
    "        age_value = 'N/A'\n",
    "        for p in p_tags:\n",
    "            if 'Age:' in p.text:\n",
    "                age_span = p.find('span', itemprop='suggestedMinAge')\n",
    "                if age_span:\n",
    "                    age_text = age_span.text.strip().replace('+', '')\n",
    "                    try:\n",
    "                        age_value = int(age_text)\n",
    "                    except ValueError:\n",
    "                        age_value = 'N/A'\n",
    "                break  # No need to keep looping once we found it\n",
    "\n",
    "        age_list.append(age_value)\n",
    "\n",
    "\n",
    "\n",
    "        # ====== MIN and MAX NUMBER OF PLAYERS ====== \n",
    "        min_players = 'N/A'\n",
    "        max_players = 'N/A'\n",
    "\n",
    "        # Find the <p> tag with class 'gameplay-item-primary mb-0' that contains the player info\n",
    "        p_tags = soup2.find_all('p', class_='gameplay-item-primary mb-0')\n",
    "\n",
    "        for p in p_tags:\n",
    "            if 'Players' in p.text:\n",
    "                # Find the meta tags inside it\n",
    "                min_tag = p.find('meta', itemprop='minValue')\n",
    "                max_tag = p.find('meta', itemprop='maxValue')\n",
    "\n",
    "                if min_tag and min_tag.has_attr('content'):\n",
    "                    min_players = int(min_tag['content'])\n",
    "\n",
    "                if max_tag and max_tag.has_attr('content'):\n",
    "                    max_players = int(max_tag['content'])\n",
    "\n",
    "                # Add them to your lists\n",
    "                min_players_list.append(min_players)\n",
    "                max_players_list.append(max_players)\n",
    "                break  # No need to continue after we found 'Players'\n",
    "\n",
    "\n",
    "\n",
    "        # ====== MIN and MAX NUMBER OF PLAYERS ====== \n",
    "        min_playtime = 'N/A'\n",
    "        max_playtime = 'N/A'\n",
    "\n",
    "        for p in p_tags:\n",
    "            text = p.text.strip()\n",
    "            if 'Min' in text and 'Age' not in text and 'Weight' not in text:\n",
    "                # Extract all numbers from the string (usually 1 or 2)\n",
    "                numbers = re.findall(r'\\d+', text)\n",
    "                if len(numbers) == 1:\n",
    "                    try:\n",
    "                        min_playtime = max_playtime = int(numbers[0])\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                elif len(numbers) >= 2:\n",
    "                    try:\n",
    "                        min_playtime = int(numbers[0])\n",
    "                        max_playtime = int(numbers[1])\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                break  # Found playtime, no need to continue\n",
    "\n",
    "        # Append to final lists\n",
    "        min_time_list.append(min_playtime)\n",
    "        max_time_list.append(max_playtime)\n",
    "\n",
    "\n",
    "\n",
    "        # ====== BOARD GAME TYPE/TYPES ====== \n",
    "        # Default values\n",
    "        game_types = ['N/A', 'N/A']\n",
    "\n",
    "        # Find all <a> tags with href that contains '/boardgamesubdomain'\n",
    "        type_tags = soup2.select('a[href^=\"/boardgamesubdomain/\"]')\n",
    "\n",
    "        # Extract the type names (text of <a> tags)\n",
    "        type_names = [a.text.strip() for a in type_tags]\n",
    "\n",
    "        # Keep only the first two\n",
    "        for j in range(min(2, len(type_names))):\n",
    "            game_types[j] = type_names[j]\n",
    "\n",
    "        # Append to your lists\n",
    "        type1_list.append(game_types[0])\n",
    "        type2_list.append(game_types[1])\n",
    "\n",
    "\n",
    "\n",
    "        # ====== DESIGNERS ====== \n",
    "        # Find all designer <a> tags\n",
    "        designer_links = soup2.find_all('a', href=lambda x: x and '/boardgamedesigner/' in x)\n",
    "        designers = [a.get_text(strip=True) for a in designer_links]\n",
    "\n",
    "        # Keep only first 5 designers\n",
    "        designers = designers[:5]\n",
    "\n",
    "        # Pad with N/A if fewer than 5\n",
    "        designers.extend(['N/A'] * (5 - len(designers)))\n",
    "        designer_rows.append(designers)\n",
    "\n",
    "\n",
    "        # ====== ARTISTS ====== \n",
    "        # Find all artist <a> tags\n",
    "        artist_links = soup2.find_all('a', href=lambda x: x and '/boardgameartist/' in x)\n",
    "        artists = [a.get_text(strip=True) for a in artist_links]\n",
    "\n",
    "        # Keep only first 5 artists\n",
    "        artists = artists[:5]\n",
    "\n",
    "        # Pad with N/A if fewer than 5\n",
    "        artists.extend(['N/A'] * (5 - len(artists)))\n",
    "        artist_rows.append(artists)\n",
    "\n",
    "\n",
    "        # ====== PUBLISHERS ====== \n",
    "        # Find all publisher <a> tags\n",
    "        publisher_links = soup2.find_all('a', href=lambda x: x and '/boardgamepublisher/' in x)\n",
    "        publishers = [a.get_text(strip=True) for a in publisher_links]\n",
    "\n",
    "        # Keep only first 5 publishers\n",
    "        publishers = publishers[:5]\n",
    "\n",
    "        # Pad with N/A if fewer than 5\n",
    "        publishers.extend(['N/A'] * (5 - len(publishers)))\n",
    "        publisher_rows.append(publishers)\n",
    "\n",
    "\n",
    "\n",
    "    # ============== DataFrames Creation ==============\n",
    "    # 1) bg_info_df\n",
    "    bg_info_df = pd.DataFrame({\n",
    "        'Title': titles_list,\n",
    "        'Geek rating': geek_rating,\n",
    "        'Avg rating': avg_rating,\n",
    "        'Num of voters': voters,\n",
    "        'Price': price_list,\n",
    "        'Year': year_list,\n",
    "        'Complexity': weight_list,\n",
    "        'Min players': min_players_list,\n",
    "        'Max players': max_players_list,\n",
    "        'Min time': min_time_list,\n",
    "        'Max time': max_time_list,\n",
    "        'Min age': age_list,\n",
    "        'Type 1': type1_list,\n",
    "        'Type 2': type2_list\n",
    "    })\n",
    "\n",
    "\n",
    "    # Convert titles_list to a DataFrame in order to use it in the next dataframes\n",
    "    titles_df = pd.DataFrame({'Title': titles_list})\n",
    "\n",
    "    # 2) bg_designers_df\n",
    "    bg_designers_df = pd.DataFrame(\n",
    "    designer_rows,\n",
    "    columns=['Designer 1', 'Designer 2', 'Designer 3', 'Designer 4', 'Designer 5'])\n",
    "    # Concatenate titles with designers DataFrame\n",
    "    bg_designers_df = pd.concat([titles_df, bg_designers_df], axis=1)\n",
    "\n",
    "    # 3) bg_artists_df\n",
    "    bg_artists_df = pd.DataFrame(\n",
    "    designer_rows,\n",
    "    columns=['Designer 1', 'Designer 2', 'Designer 3', 'Designer 4', 'Designer 5'])\n",
    "    bg_artists_df = pd.concat([titles_df, bg_artists_df], axis=1)\n",
    "\n",
    "    # 4) bg_publishers_df\n",
    "    bg_publishers_df = pd.DataFrame(\n",
    "    designer_rows,\n",
    "    columns=['Designer 1', 'Designer 2', 'Designer 3', 'Designer 4', 'Designer 5'])\n",
    "    bg_publishers_df = pd.concat([titles_df, bg_publishers_df], axis=1)\n",
    "\n",
    "\n",
    "    # ============== CSV creation ==============\n",
    "    # Create folders if they don't exist\n",
    "    os.makedirs('info', exist_ok=True)\n",
    "    os.makedirs('designers', exist_ok=True)\n",
    "    os.makedirs('artists', exist_ok=True)\n",
    "    os.makedirs('publishers', exist_ok=True)\n",
    "\n",
    "    # Save CSVs inside their respective folders\n",
    "    bg_info_df.to_csv(f'info/bg_info_{i:03}.csv', index=False)\n",
    "    bg_designers_df.to_csv(f'designers/bg_designers_{i:03}.csv', index=False)\n",
    "    bg_artists_df.to_csv(f'artists/bg_artists_{i:03}.csv', index=False)\n",
    "    bg_publishers_df.to_csv(f'publishers/bg_publishers_{i:03}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "    page_end_time = time.time()\n",
    "    print(f'Page scrape time: {int((page_end_time - page_start_time) / 60)} minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de25ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will merge the created csv files to a single csv (for each part)\n",
    "\n",
    "# 1) we create the bg_info.csv\n",
    "folder = \"info\" \n",
    "# list files inside the subfolder\n",
    "files = os.listdir(folder)\n",
    "csv_files = [f for f in files if f.startswith(\"bg_info_\") and f.endswith(\".csv\")]\n",
    "csv_files.sort()  # keep them in order\n",
    "# prepend folder name so pandas can find them\n",
    "df_list = [pd.read_csv(os.path.join(folder, f)) for f in csv_files]\n",
    "# merge\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "# save merged file in the main folder \n",
    "merged_df.to_csv(\"bg_info.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# 2) we create the bg_artists.csv\n",
    "folder = \"artists\" \n",
    "# list files inside the subfolder\n",
    "files = os.listdir(folder)\n",
    "csv_files = [f for f in files if f.startswith(\"bg_artists_\") and f.endswith(\".csv\")]\n",
    "csv_files.sort() \n",
    "# prepend folder name so pandas can find them\n",
    "df_list = [pd.read_csv(os.path.join(folder, f)) for f in csv_files]\n",
    "# merge\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "# save merged file in the main folder \n",
    "merged_df.to_csv(\"bg_artists.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# 3) we create the bg_designers.csv\n",
    "folder = \"designers\" \n",
    "# list files inside the subfolder\n",
    "files = os.listdir(folder)\n",
    "csv_files = [f for f in files if f.startswith(\"bg_designers_\") and f.endswith(\".csv\")]\n",
    "csv_files.sort()  \n",
    "# prepend folder name so pandas can find them\n",
    "df_list = [pd.read_csv(os.path.join(folder, f)) for f in csv_files]\n",
    "# merge\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "# save merged file in the main folder \n",
    "merged_df.to_csv(\"bg_designers.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# 4) we create the bg_publishers.csv\n",
    "folder = \"publishers\" \n",
    "# list files inside the subfolder\n",
    "files = os.listdir(folder)\n",
    "csv_files = [f for f in files if f.startswith(\"bg_publishers_\") and f.endswith(\".csv\")]\n",
    "csv_files.sort()  \n",
    "# prepend folder name so pandas can find them\n",
    "df_list = [pd.read_csv(os.path.join(folder, f)) for f in csv_files]\n",
    "# merge\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "# save merged file in the main folder \n",
    "merged_df.to_csv(\"bg_publishers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764fe493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312_all_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
